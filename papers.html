<!DOCTYPE html>
<html lang="en">
<head>
    <title>Maggie's Homepage</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="author" content="Xiaoning Liu">
    <meta name="keywords" content="Xiaoning Liu, Maggie Liu, Xiaoning Liu RMIT, Maggie Liu RMIT, Xiaoning Maggie Liu RMIT">
    <meta name="robots" content="index,follow">
    <meta name="description" content="Homepage of Maggie Liu">
    <link href="https://fonts.googleapis.com/css?family=Lato:100,300,400,700,900" rel="stylesheet">
    <link rel="stylesheet" type="text/css" media="screen,print" href="css/style.css" />
    <link href="css/bootstrap.min.css" rel="stylesheet" media="screen" />

</head>

<body>
<div class="container">
<br>
<br>

<h3>SC<sup>2</sup>ale: Secure Collaborative Computation at Scale</h3>
<p style="font-style: italic;
  font-size: 20px;
  line-height: 1.4;
  margin-top: 0;
  margin-bottom: 10px;">On the design, analysis, and fielding of secure computation<p>



<hr/>
<h3>
  <a href="index.html">Home</a> /
  <a href="papers.html">Research</a> /
  <a href="people.html">People</a> /
  <a href="teaching.html">Teaching</a> /
  <a href="service.html">Service</a> /
  <a href="talk.html">Talk</a> /
  <a href="award.html">Award</a>
</h3>
<hr/>



<script>
function copy(dest, source) {
  if(dest.source == source) {
    dest.innerHTML = "";
    dest.source = null;
    dest.style.width="0px";
    dest.style.border = "";
    dest.style.padding = "0px";
  }
  else {
    dest.innerHTML = source.innerHTML;
    dest.source = source;
    dest.style.width = "800px";
    dest.style.padding = "10px";
    dest.style.border = "2px dotted gray";
    dest.style.background = "#F5F5F5";
    dest.style.margin = "10px";
  }
  dest.blur();
}
</script>


<script>
        paper_count = 0

        function add_paper(title, authors, conference, link, bib, abstract, pdf, code, arxiv_link, slides, talk, msg) {
            list_entry = "<li style=\"font-size:18px\">"
            if (link != null)
                list_entry += "<a href=\"" + link + "\" target='_blank'>"
            list_entry += "<b>" + title + "</b>"
            if (link != null)
                list_entry += "</a>"
            list_entry += "<br>" + authors + ".<br>" + conference + ".</li>"

            if (bib != null) {
                list_entry += "<div id=\"bib" + paper_count + "\" style=\"display:none\">" + bib + "</div>"
                list_entry += "<a href=\"javascript:copy(div" + paper_count + ",bib" + paper_count + ")\"> <span class=\"label label-success\">bib</span></a>"
            }

            if (abstract != null) {
                list_entry += "<div id=\"abstract" + paper_count + "\" style=\"display:none\">" + abstract + "</div>"
                list_entry += "<a href=\"javascript:copy(div" + paper_count + ",abstract" + paper_count + ")\"> <span class=\"label label-warning\">abstract</span></a>"
            }
            if (pdf != null)
                list_entry += " <a href=\"" + pdf + "\"><span class=\"label label-primary\">PDF</span></a>"

            if (code != null)
                list_entry += " <a href=\"" + code + "\"><span class=\"label label-danger\">code/models</span></a>"

           if (arxiv_link != null)
                list_entry += " <a href=\"" + arxiv_link + "\"><span class=\"label label-primary\">arXiv</span></a>"

            if (slides != null)
                list_entry += " <a href=\"" + slides + "\"><span class=\"label label-info\">slides/poster</span></a>"

            if (talk != null)
                list_entry += " <a href=\"" + talk + "\"><span class=\"label label-success\">talk</span></a>"

            list_entry += "<br>"

            if (msg != null)
                list_entry += "<i>" + msg + "</i>"

            list_entry += "<div id=\"div" + paper_count + "\" style=\"font-size:15px\"></div><br>"

            document.write(list_entry)

            paper_count += 1
        }

        document.write("<details open><summary><p><h3>&#10148; Privacy Preserving Machine Learning</h3></p></summary>")

        document.write("<ul>")

        add_paper("SIGuard: Guarding Secure Inference with Post Data Privacy",
            "Xinqian Wang, <b>Xiaoning Liu</b>, Shangqi Lai, Xun Yi, and Xingliang Yuan",
            "In the Network and Distributed System Security Symposium (NDSS), 2025",
            "https://www.ndss-symposium.org/ndss-paper/siguard-guarding-secure-inference-with-post-data-privacy/",//link,
            null,//bib
            "Secure inference is designed to enable encrypted machine learning model prediction over encrypted data. It will ease privacy concerns when models are deployed in Machine Learning as a Service (MLaaS). For efficiency, most of recent secure inference protocols are constructed using secure multi-party computation (MPC) techniques. They can ensure that MLaaS computes inference without knowing the inputs of users and model owners. However, MPC-based protocols do not hide information revealed from their output. In the context of secure inference, prediction outputs (i.e., inference results of encrypted user inputs) are revealed to the users. As a result, adversaries can compromise textbf{output privacy} of secure inference, i.e., launching Membership Inference Attacks (MIAs) by querying encrypted models, just like MIAs in plaintext inference. We observe that MPC-based secure inference often yields perturbed predictions due to approximations of nonlinear functions like softmax compared to its plaintext version on identical user inputs. Thus, we evaluate whether or not MIAs can still exploit such perturbed predictions on known secure inference protocols. Our results show that secure inference remains vulnerable to MIAs. The adversary can steal membership information with high successful rates comparable to plaintext MIAs. To tackle this open challenge, we propose textbf{SIGuard}, a framework to guard the output privacy of secure inference from being exploited by MIAs. textbf{SIGuard}'s protocol can seamlessly be integrated into existing MPC-based secure inference protocols without intruding on their computation. It proceeds with encrypted predictions outputted from secure inference, and then crafts noise for perturbing encrypted predictions without compromising inference accuracy; only the perturbed predictions are revealed to users at the end of protocol execution. textbf{SIGuard} achieves stringent privacy guarantees via a co-design of MPC techniques and machine learning. We further conduct comprehensive evaluations to find the optimal hyper-parameters for balanced efficiency and defense effectiveness against MIAs. Together, our evaluation shows textbf{SIGuard} effectively defends against MIAs by reducing the attack accuracy to be around the random guess with overhead (1.1s), occupying ~24.8% of secure inference (3.29s) on widely used ResNet34 over CIFAR-10.",//abstract    
            "papers/siguard.pdf",//paper
            null,
            null,
            null,
            null
        )

        add_paper("Privacy-Preserving Automated Deep Learning for Secure Inference Service",
            "Fuyi Wang, Jinzhi Ouyang, Leo Yu Zhang, Lei Pan, Shengshan Hu, <b>Xiaoning Liu</b>, Robin Doss",
            "IEEE Transactions on Dependable and Secure Computing, 2025",
            "https://ieeexplore.ieee.org/abstract/document/11177552/",//link "https://doi.org/10.1016/j.cose.2023.103406",
            null,//bib
            null,//abstract    
            null,//paper
            null,
            null,
            null,
            null
        )

        add_paper("MedShield: A Fast Cryptographic Framework for Private Multi-Service Medical Diagnosis",
            "Fuyi Wang, Jinzhi Ouyang, <b>Xiaoning Liu</b>, Lei Pan, Leo Yu Zhang, Robin Doss",
            "IEEE Transactions on Services Computing, 2025",
            "https://ieeexplore.ieee.org/abstract/document/10829796/",//link "https://doi.org/10.1016/j.cose.2023.103406",
            null,//bib
            null,//abstract    
            null,//paper
            null,
            null,
            null,
            null
        )

        add_paper("GoCrowd: Obliviously Aggregating Crowd Wisdom with Quality Awareness in Crowdsourcing",
            "<b>Xiaoning Liu</b>, Yifeng Zheng, Xingliang Yuan, and Xun Yi",
            "IEEE Transactions on Dependable and Secure Computing (TDSC), accepted, 2024",
            "https://www.computer.org/csdl/journal/tq/5555/01/10559394/1XR0jDBDAqI", //link
            "@article{liu2024gocrowd, title={GoCrowd: Obliviously Aggregating Crowd Wisdom With Quality Awareness in Crowdsourcing}, author={Liu, Xiaoning and Zheng, Yifeng and Yuan, Xingliang and Yi, Xun}, journal={IEEE Transactions on Dependable and Secure Computing}, year={2024}, publisher={IEEE} }", //citation
            "Organizations these days capitalize on crowdsourcing to learn collective wisdom from a population of individuals. Vast amounts of data have been gathered, making the crowdsourcing platforms a lucrative target to steal data from and thus raising severe privacy concerns. Data contributed by workers may carry sensitive individual information. Meanwhile, organizations deem the aggregate statistics as intellectual property. In this paper, we propose, design, and evaluate GoCrowd, a system framework for obliviously aggregating wisdom with quality assurance in crowdsourcing. At its core, we propose constructions for two procedures. The starting point is a gold-standard based private worker quality control procedure that provides privacy-friendly worker quality assurance under the widely popular gold-standard mechanism. The subsequent procedure is an oblivious wisdom aggregation procedure that obliviously learns aggregate statistics over workers' data while considering their quality. We securely realize these procedures with only lightweight secret sharing techniques. Our system is utterly oblivious to the service provider, and ensures that only the requester can learn the aggregate quality-aware statistics but nothing more. Extensive evaluations show that GoCrowd can produce quality statistics over data from 500 workers for 200 16-choice questions within 1 s.", //abstract
            null, //paper
            null, 
            null,
            null,
            null
        )
    

    
        add_paper("OblivGNN: Oblivious Inference on Transductive and Inductive Graph Neural Network",
            "Zhibo Xu, Shangqi Lai, <b>Xiaoning Liu</b>, Alsharif Abuadbba, Xingliang Yuan, and Xun Yi",
            "In the 33rd USENIX Security Symposium, 2024, (acceptance ratio: 17%)",
            "https://www.usenix.org/conference/usenixsecurity24/presentation/xu-zhibo",//link
            "@inproceedings{xu2024oblivgnn, title={$\{$OblivGNN$\}$: Oblivious Inference on Transductive and Inductive Graph Neural Network}, author={Xu, Zhibo and Lai, Shangqi and Liu, Xiaoning and Abuadbba, Alsharif and Yuan, Xingliang and Yi, Xun}, booktitle={33rd USENIX Security Symposium (USENIX Security 24)}, pages={2209--2226}, year={2024}}",//citation
            "Graph Neural Networks (GNNs) have emerged as a powerful tool for analysing graph-structured data across various domains, including social networks, banking, and bioinformatics. In the meantime, graph data contains sensitive information, such as social relations, financial transactions, and chemical structures, and GNN models are IPs of the model owner. Thus, deploying GNNs in cloud-based Machine Learning as a Service (MLaaS) raises significant privacy concerns. In this paper, we present a comprehensive solution to enable secure GNN inference in MLaaS, named OblivGNN. OblivGNN is designed to support both transductive (static graph) and inductive (dynamic graph) inference services without revealing either graph data or GNN models. In particular, we adopt a lightweight cryptographic primitive, i.e., function secret sharing, to achieve low communication and computation overhead during inference. Furthermore, we are the first to propose a secure update protocol for the inductive setting, which can obliviously update the graph without revealing which parts of the graph are updated. Particularly, our results with three widely-used graph datasets (Cora, Citeseer, and Pubmed) show that OblivGNN can achieve comparable accuracy to an Additive Secret Sharing-based baseline. Nonetheless, our design reduces the runtime cost by up to 38% and the communication cost by 10× to 151×, highlighting its practicality when processing large graphs with GNN models.",//abstract
            "papers/oblivgnn.pdf",
            null,
            null,
            null,
            null
        )
        
        add_paper("Model Extraction Attacks on Privacy-Preserving Deep Learning based Medical Services",
            "Xinqian Wang, <b>Xiaoning Liu</b>, Xun Yi, Xuechao Yang, and Iqbal Gondal",
            "In the 25th International Conference on Web Information Systems Engineering (WISE), 2024",
            null,//link
            null,
            null,
            null,
            null,
            null,
            null,
            null
        )
    
        add_paper("Model Extraction Attack on MPC Hardened Vertical Federated Learning",
            "Xinqian Wang, <b>Xiaoning Liu</b>, and Xun Yi",
            "In the 18th International Conference on Provable and Practical Security (ProvSec), 2024",
            null,//link
            null,
            null,
            null,
            null,
            null,
            null,
            null
        )
        add_paper("TrustMIS: Trust-Enhanced Inference Framework for Medical Imaging Segmentation",
            "Fuyi Wang, Jinzhi Ouyang, Lei Pan, Leo Yu Zhang, <b>Xiaoning Liu</b>, Yanping Wang and Robin Doss",
            "In the European Conference on Artificial Intelligence (ECAI), 2024",
            null,//link
            null,
            null,
            null,
            null,
            null,
            null,
            null
        )
   
        add_paper("MediSC: Towards Secure and Lightweight Deep Learning as a Medical Diagnostic Service",
            "<b>Xiaoning Liu</b>, Yifeng Zheng, Xingliang Yuan, and Xun Yi",
            "In the 26th European Symposium on Research in Computer Security (ESORICS), 2021, (the only one <b>Best Paper Award</b> of 351 submissions, acceptance ratio: 71/351=20.2%)",
            null,//link
            "@inproceedings{liu2021medisc,title={Medisc: Towards secure and lightweight deep learning as a medical diagnostic service},author={Liu, Xiaoning and Zheng, Yifeng and Yuan, Xingliang and Yi, Xun},booktitle={Proc. of ESORICS},year={2021}}",
            "The striking progress of deep learning paves the way towards intelligent and quality medical diagnostic services. Enterprises deploy such services via the neural network (NN) inference, yet confronted with rising privacy concerns of the medical data being diagnosed and the pretrained NN models. We propose MediSC, a system framework that enables enterprises to offer secure medical diagnostic service to their customers via an execution of NN inference in the ciphertext domain. MediSC ensures the privacy of both parties with cryptographic guarantees. At the heart, we present an efficient and communication-optimized secure inference protocol that purely relies on the lightweight secret sharing techniques and can well cope with the commonly-used linear and non-linear NN layers. Compared to the garbled circuits based solutions, the latency and communication of MediSC are 24x lower and 868x less for the secure ReLU, and 20x lower and 314x less for the secure Max-pool. We evaluate MediSC on two benchmark and four real-world medical datasets, and comprehensively compare it with prior arts. The results demonstrate the promising performance of MediSC, which is much more bandwidthefficient compared to prior works.",
            "papers/medisc.pdf",
            null,
            null,
            null,
            null
        )
         
        add_paper("MUD-PQFed: Towards Malicious User Detection in Privacy-Preserving Quantized Federated Learning",
            "Hua Ma, Qun Li, Yifeng Zheng, Zhi Zhang, <b>Xiaoning Liu</b>, Yansong Gao, Said F. Al-Sarawi, and Derek Abbott",
            "Computer & Security, 2023",
            null,//link "https://doi.org/10.1016/j.cose.2023.103406",
            null,//bib
            null,//abstract    
            null,//paper
            null,
            null,
            null,
            null
        )
         add_paper("Securely Outsourcing Neural Network Inference to the Cloud with Lightweight Techniques",
            "<b>Xiaoning Liu</b>, Yifeng Zheng, Xingliang Yuan, and Xun Yi",
            "IEEE Transactions on Dependable and Secure Computing (TDSC), accepted, 2022",
            null,
            "@article{liu2022securely,title={Securely Outsourcing Neural Network Inference to the Cloud with Lightweight Techniques},author={Liu, Xiaoning and Zheng, Yifeng and Yuan, Xingliang and Yi, Xun},journal={IEEE Transactions on Dependable and Secure Computing}, year={2022},publisher={IEEE}}",
            "Neural network (NN) inference services enrich many applications, like image classification, object recognition, facial verification, and more. These NN inference services are increasingly becoming an essential offering from cloud computing providers, where end-users' data are offloaded to the cloud for inference under a customized model. However, current cloud-based inference services operate on clear inputs and NN models, raising paramount privacy concerns. Individual user data may contain private information that should always remain confidential. Meanwhile, the NN model is deemed proprietary to the model owner as model training requires substantial resources. In this paper, we present, tailor, and evaluate Sonic, a lightweight secure NN inference service delegated in the cloud. Sonic leverages the cloud computing paradigm to fully outsource the secure inference, freeing end devices and model owners from being actively online for assistance. Sonic guards both user input and model privacy along the whole service flow. We design a series of secure and efficient NN layer functions purely using lightweight cryptographic primitives. Extensive evaluations demonstrate that Sonic achieves up to 60x bandwidth saving in online inference compared to prior art.",
            "papers/sonic.pdf",
            null,
            null,
            null,
            null
        )
        add_paper("Deep Learning-Based Medical Diagnostic Services: A Secure, Lightweight, and Accurate Realization",
            "<b>Xiaoning Liu</b>, Yifeng Zheng, Xingliang Yuan, and Xun Yi",
            "Journal of Computer Security (JCS), invited paper by ESORICS'21, 2022",
            null,//link
            null,//bib
            null,//abstract
            "papers/cryptmed.pdf",
            null,
            null,
            null,
            null
        )

       
        add_paper("Leia: A Lightweight Cryptographic Neural Network Inference System at the Edge",
            "<b>Xiaoning Liu</b>, Bang Wu, Xingliang Yuan, and Xun Yi",
            "IEEE Transactions on Information Forensics and Security (TIFS), accepted, 2021",
            null,
            "@article{liu2021leia,\n\t title={Leia: A lightweight cryptographic neural network inference system at the edge}, author={Liu, Xiaoning and Wu, Bang and Yuan, Xingliang and Yi, Xun}, journal={IEEE Transactions on Information Forensics and Security}, volume={17},pages={237--252}, year={2021}, publisher={IEEE}}",
            "The advances in machine learning have revealed its great potential for emerging mobile applications such as face recognition and voice assistant. Models trained via a Neural Network (NN) can offer accurate and efficient inference services for mobile users. Unfortunately, the current deployment of such service encounters privacy concerns. Directly offloading the model to the mobile device violates model privacy of the model owner, while feeding user input to the service compromises user privacy. To address this issue, we propose Leia, a lightweight cryptographic NN inference system at the edge. Leia is designed from two mobile-friendly perspectives. First, it leverages the paradigm of edge computing wherein the inference procedure keeps the model closer to the mobile user to foster low latency service. Specifically, Leia’s architecture consists of two non-colluding edge services to obliviously perform NN inference on the encoded user data and model. Second, Leia’s realization makes the judicious use of potentially constrained computational and communication resources in edge devices. We adapt the Binarized Neural Network (BNN), a trending flavor of NN with low inference overhead, and purely choose the lightweight secret sharing techniques to realize secure blocks of BNN. We implement Leia and deploy it on Raspberry Pi. Empirical evaluations on benchmark and medical datasets via various models demonstrate the practicality of Leia.",
            "papers/leia.pdf",
            null,
            null,
            null,
            null
        )
        document.write("</ul></details>")
       









document.write("<details open><summary><p><h3>&#10148; Secure Collaborative Stream Analytics</h3></p></summary>")
        document.write("<ul>")
        add_paper("Privacy-Preserving Collaborative Analytics on Medical Time Series Data",
            "<b>Xiaoning Liu</b>, Yifeng Zheng, Xun Yi, and Surya Nepal",
            "IEEE Transactions on Dependable and Secure Computing (TDSC), accepted, 2020",
            null,
            "@article{liu2020privacy, title={Privacy-preserving collaborative analytics on medical time series data}, author={Liu, Xiaoning and Zheng, Yifeng and Yi, Xun and Nepal, Surya}, journal={IEEE Transactions on Dependable and Secure Computing},  year={2020}, publisher={IEEE}}",
            "Medical time series data analytics based on dynamic time warping (DTW) greatly benefits modern medical research. Driven by the distributed nature of medical data, the collaboration of multiple healthcare institutions is usually necessary for a sound medical conclusion. Among others, a typical use case is disease screening for public health, where multiple healthcare institutions wish to collaboratively detect over their joint datasets the patients whose medical records have similar features to the given query samples. However, sharing the medical data faces critical privacy obstacles with the increasingly strict legal regulations on data privacy. In this article, we present the design of a novel system enabling privacy-preserving DTW-based analytics on distributed medical time series datasets. Our system is built from a delicate synergy of techniques from both cryptography and data mining domains, where the key idea is to leverage observations on the advancements in plaintext DTW analytics (e.g., clustering and pruning) to facilitate the scalable computation in the ciphertext domain, through our tailored security design. Extensive experiments over real medical time series datasets demonstrate the promising performance of our system, e.g., our system is able to process a secure DTW query computation over 15K time series sequences in 34 minutes.",
            "papers/dtw-tdsc.pdf",
            null,
            null,
            null,
            null
        )

        add_paper("Privacy-preserving collaborative medical time series analysis based on dynamic time warping",
            "<b>Xiaoning Liu</b> and Xun Yi",
            "In the 24th European Symposium on Research in Computer Security (ESORICS), Luxembourg, 2019. (acceptance ratio: 67/344= 19.5%)",
            null,
            "@inproceedings{liu2019privacy, title={Privacy-preserving collaborative medical time series analysis based on dynamic time warping}, author={Liu, Xiaoning and Yi, Xun}, booktitle={European Symposium on Research in Computer Security}, pages={439--460}, year={2019}, organization={Springer}}",
            "Evaluating medical time series (e.g., physiological sequences) under dynamic time warping (DTW) derives insights assisting biomedical research and clinical decision making. Due to the natural distribution of medical data, a collaboration among multiple healthcare institutes is required to carry out a reliable and quality medical judgment. Yet sharing medical data cross the boundaries of multiple institutions faces widespread privacy threats, along with increasingly stringent laws and privacy regulations nowadays. Addressing such demands, we propose a privacy-preserving system tailored for the DTW-based analysis over the decentralized medical time series sequences. Our system constructs a secure and scalable architecture to deliver comprehensive results from a joint data analytic task with privacy preservation. To accelerate complicated DTW query processing, our system adapts the advancement in secure multi-party computation (MPC) framework to realize encrypted DTW computation, decomposing complicated and iterative operations into atomic functions under suitable MPC primitives and optimized for DTW. Moreover, our system introduces a secure hybrid pruning strategy that diminishes the volume of time series sequences that are submitted before and processed within the encrypted DTW query. We implement a prototype and evaluate its performance on Amazon Cloud. The empirical evaluation demonstrates the feasibility of our system in practice.",
            "papers/dtw-esorics.pdf",
            null,
            null,
            null,
            null
        )



        add_paper("Towards Privacy-Preserving Forensic Analysis for Time-series Medical Data",
            "<b>Xiaoning Liu</b>, Xingliang Yuan, and Joseph Liu",
            "in the 17th IEEE International Conference on Trust, Security and Privacy in Computing And Communications (TrustCom), 2018, New York, USA",
            null,
            "@inproceedings{liu2018towards, title={Towards privacy-preserving forensic analysis for time-series medical data}, author={Liu, Xiaoning and Yuan, Xingliang and Liu, Joseph}, booktitle={2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And  Communications/12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)}, pages={1664--1668}, year={2018}, organization={IEEE}}",
            null,
            "papers/trustcom.pdf",
            null,
            null,
            null,
            null
        )



        add_paper("EncSIM: An encrypted similarity search service for distributed high-dimensional datasets",
        "<b>Xiaoning Liu</b>, Xingliang Yuan, and Cong Wang",
        "In the 2017 IEEE/ACM 25th International Symposium on Quality of Service (IWQoS), 2017, Barcelona, Spain. (acceptance ratio 19.5%)",
        null,
        "@inproceedings{liu2017encsim, title={EncSIM: An encrypted similarity search service for distributed high-dimensional datasets}, author={Liu, Xiaoning and Yuan, Xingliang and Wang, Cong}, booktitle={2017 IEEE/ACM 25th International Symposium on Quality of Service (IWQoS)}, pages={1--10}, year={2017}, organization={IEEE}}",
        null,
        "papers/encsim.pdf",
        null,
        null,
        null,
        null
    )
document.write("</ul></details>")




document.write("<details open><summary><p><h3>&#10148; Machine Learning Security and Privacy</h3></p></summary>")
document.write("<ul>")

        add_paper("Imprint of the Forgotten: Stealthy Membership Inference In Unlearned Graph Neural Networks",
        "He Zhang, Bang Wu, <b>Xiaoning Liu</b>, Karin Verspoor, Xun Yi",
        "In the 40th AAAI Conference on Artificial Intelligence (AAAI),  2026, Singapore.",
        "https://openreview.net/pdf?id=ikT9q8cdq9",//link
        null, //bib
        "Graphs effectively model interactions in real-world applications such as social and trade networks, where Graph Neural Networks (GNNs) excel at tasks such as link prediction to enhance user experiences. Despite these benefits, users raise privacy concerns as user data can be exploited to improve GNN performance without consent. Accordingly, various graph unlearning methods have been developed. Prior work shows that comparing models before and after unlearning enables attackers to launch former membership inference attacks (FMIA) on unlearned data. However, the imprint of unlearned data left in the unlearned model itself remains underexplored, and existing membership inference methods mainly exploit overfitting, making them ineffective for identifying unlearned data. To address this, we conducted theoretical analysis and proposed an attack framework targeting unlearned GNNs by learning the distribution patterns of unlearned data to distinguish them from normal test data. Extensive experiments on four real-world datasets and GNN architectures confirm our framework's effectiveness and reveal significant vulnerabilities in current graph unlearning methods.", //abstract
        null, //pdf
        null, //code
        null, //arxiv
        null, //slides
        null, //talk
        null //msg
        ) //title, authors, conference, link, bib, abstract, pdf, code, arxiv_link, slides, talk, msg

        add_paper("Dynamic graph unlearning: a general and efficient post-processing method via gradient transformation",
        "He Zhang, Bang Wu, Xiangwen Yang, Xingliang Yuan, <b>Xiaoning Liu</b>, Xun Yi",
        "In the ACM on Web Conference (WWW) 2025, Sydney, Australia.",
        "https://dl.acm.org/doi/abs/10.1145/3696410.3714911",//link
        null, //bib
        "Dynamic graph neural networks (DGNNs) have emerged and been widely deployed in various web applications (e.g., Reddit) to serve users (e.g., personalized content delivery) due to their remarkable ability to learn from complex and dynamic user interaction data. Despite benefiting from high-quality services, users have raised privacy concerns, such as misuse of personal data (e.g., dynamic user-user/item interaction) for model training, requiring DGNNs to forget their data to meet AI governance laws (e.g., the right to be forgotten in GDPR). However, current static graph unlearning studies cannot unlearn dynamic graph elements and exhibit limitations such as the model-specific design or reliance on pre-processing, which disenable their practicability in dynamic graph unlearning. To this end, we study the dynamic graph unlearning for the first time and propose an effective, efficient, general, and post-processing method to implement DGNN unlearning. Specifically, we first formulate dynamic graph unlearning in the context of continuous-time dynamic graphs, and then propose a method called Gradient Transformation that directly maps the unlearning request to the desired parameter update. Comprehensive evaluations on six real-world datasets and state-of-the-art DGNN backbones demonstrate its effectiveness (e.g., limited drop or obvious improvement in utility) and efficiency (e.g., 7.23× speed-up) advantages. Additionally, our method has the potential to handle future unlearning requests with significant performance gains (e.g., 32.59× speed-up).",
        null, //pdf
        null, //code
        null, //arxiv
        null, //slides
        null, //talk
        null //msg
        ) //title, authors, conference, link, bib, abstract, pdf, code, arxiv_link, slides, talk, msg

        add_paper("Unsupervised Backdoor Detection and Mitigation for Spiking Neural Networks",
        "Jiachen Li, Bang Wu, Xiaoyu Xia, <b>Xiaoning Liu</b>, Xun Yi, Xiuzhen Zhang",
        "In the 28th International Symposium on Research in Attacks, Intrusions and Defenses (RAID),  2025, Gold Coast, Australia.",
        null,//link
        null, //bib
        "Spiking Neural Networks (SNNs) have gained increasing attention for their superior energy efficiency compared to Artificial Neural Networks (ANNs). However, their security aspects, particularly under backdoor attacks, have received limited attention. Existing defense methods developed for ANNs perform poorly or can be easily bypassed in SNNs due to their event-driven and temporal dependencies. This paper identifies the key blockers that hinder traditional backdoor defenses in SNNs and proposes an unsupervised post-training detection framework, Temporal Membrane Potential Backdoor Detection (TMPBD), to overcome these challenges. TMPBD leverages the maximum margin statistics of temporal membrane potential (TMP) in the final spiking layer to detect target labels without any attack knowledge or data access. We further introduce a robust mitigation mechanism, Neural Dendrites Suppression Backdoor Mitigation (NDSBM), which clamps dendritic connections between early convolutional layers to suppress malicious neurons while preserving benign behaviors, guided by TMP extracted from a small, clean, unlabeled dataset. Extensive experiments on multiple neuromorphic benchmarks and state-of-the-art input-aware dynamic trigger attacks demonstrate that TMPBD achieves 100% detection accuracy, while NDSBM reduces the attack success rate from 100% to 8.44%, and to 2.81% when combined with detection, without degrading clean accuracy.",
        null, //pdf
        null, //code
        null, //arxiv
        null, //slides
        null, //talk
        null //msg
        ) //title, authors, conference, link, bib, abstract, pdf, code, arxiv_link, slides, talk, msg


        add_paper("A survey on federated unlearning: Challenges, methods, and future directions",
        "Ziyao Liu, Yu Jiang, Jiyuan Shen, Minyi Peng, Kwok-Yan Lam, Xingliang Yuan, Xiaoning Liu",
        "ACM Computing Surveys, 2024",
        "https://dl.acm.org/doi/full/10.1145/3679014",//link
        null, //bib
        null, //abstract
        null, //pdf
        null, //code
        null, //arxiv
        null, //slides
        null, //talk
        null //msg
        ) //title, authors, conference, link, bib, abstract, pdf, code, arxiv_link, slides, talk, msg


document.write("</ul></details>")

</script>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
</div>
</body>
</html>
